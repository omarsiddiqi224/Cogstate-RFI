{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a49481",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f4994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = \"false\"  # Disable LangSmith tracing\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",))\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Embed & Store\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b04c30",
   "metadata": {},
   "source": [
    "### Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e86cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/flgh8s7960bc7q380pyn0c6m0000gn/T/ipykernel_63955/3486059876.py:29: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals. This process enables efficient handling of complex tasks by transforming them into multiple manageable tasks, allowing the agent to tackle each step individually.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "# 1. Prompt for generating multiple queries\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Chain to generate queries\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# 3. Function to get unique union of documents\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# 4. Build the retrieval chain\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# 5. Define the final RAG chain\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the following question based on this context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccddc63",
   "metadata": {},
   "source": [
    "### RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af1565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Docs:  15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to tackle the overall task by dividing it into more achievable steps, leading to improved performance and results.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Prompt for RAG-Fusion\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries related to: {question} \\n Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Chain to generate queries\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# 3. Reciprocal Rank Fusion function\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n",
    "# 4. Build the retrieval chain for fusion\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "print(\"No. of Docs: \", len(docs))\n",
    "\n",
    "# 5. Define the final RAG chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45d732",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291fcd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer (Recursive): \n",
      " The main components of an LLM-powered autonomous agent system provide specific functionalities as follows:\n",
      "\n",
      "1. Planning Component:\n",
      "- Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals for efficient handling of complex tasks.\n",
      "- Reflection and refinement: The agent engages in self-criticism, self-reflection over past actions, learns from mistakes, and refines actions for future steps to improve the quality of final results.\n",
      "\n",
      "2. Memory Component:\n",
      "- Short-term memory: Utilized for in-context learning to adapt to current situations.\n",
      "- Long-term memory: Enables the agent to retain and recall infinite information over extended periods, often leveraging an external vector store for fast retrieval.\n",
      "\n",
      "3. Tool Use Component:\n",
      "- The agent learns to call external APIs for extra information missing from the model weights.\n",
      "- Accesses current information, code execution capabilities, proprietary information sources, and more to enhance decision-making processes.\n",
      "\n",
      "Overall, these components work together with the LLM as the core controller to facilitate problem-solving, decision-making, and efficient task execution within the autonomous agent system.\n",
      "Answer (Individual): \n",
      " The main components of an LLM-powered autonomous agent system provide specific functionalities as follows:\n",
      "\n",
      "1. Planning Component:\n",
      "- Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals for efficient handling of complex tasks.\n",
      "- Reflection and refinement: The agent engages in self-criticism, self-reflection over past actions, learns from mistakes, and refines actions for future steps to improve the quality of final results.\n",
      "\n",
      "2. Memory Component:\n",
      "- Short-term memory: Utilized for in-context learning to adapt to current situations.\n",
      "- Long-term memory: Enables the agent to retain and recall infinite information over extended periods, often leveraging an external vector store for fast retrieval.\n",
      "\n",
      "3. Tool Use Component:\n",
      "- The agent learns to call external APIs for extra information missing from the model weights.\n",
      "- Accesses current information, code execution capabilities, proprietary information sources, and more to enhance decision-making processes.\n",
      "\n",
      "Overall, these components work together with the LLM as the core controller to facilitate problem-solving, decision-making, and efficient task execution within the autonomous agent system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. Prompt to generate sub-questions\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions from a complex question. The goal is to break down the complex question into a series of simpler questions that can be answered individually. Generate 3 sub-questions. Question: {question}\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "generate_queries_decomposition = prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "\n",
    "# 2. Prompt for recursive answering\n",
    "template = \"\"\"\n",
    "Here is the question you need to answer: {question}\n",
    "Here is any available background question + answer pairs: {q_a_pairs}\n",
    "Here is additional context relevant to the question: {context}\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question.\n",
    "\"\"\"\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 3. Recursive answering loop\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = f\"Question: {q}\\nAnswer: {answer}\\n\\n\"\n",
    "    q_a_pairs = q_a_pairs + q_a_pair\n",
    "\n",
    "print(\"Answer (Recursive): \\n\", answer)\n",
    "\n",
    "# 4. Answer each sub-question individually \n",
    "\n",
    "# RAG prompt\n",
    "prompt_template = \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt_rag = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "ind_answer = final_rag_chain.invoke({\"context\":context,\"question\":question})\n",
    "print(\"Answer (Individual): \\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285cf25",
   "metadata": {},
   "source": [
    "### Step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8fa1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Task decomposition for LLM agents involves breaking down large tasks into smaller, more manageable subgoals. This process enables the agent to efficiently handle complex tasks by dividing them into smaller steps that are easier to tackle. By decomposing tasks, LLM agents can effectively plan and execute their actions, leading to improved problem-solving capabilities and higher-quality results.\n",
      "\n",
      "One common technique used for task decomposition in LLM agents is the Chain of Thought (CoT) method, as described by Wei et al. (2022). CoT prompts the model to \"think step by step,\" encouraging it to break down hard tasks into simpler steps. This approach helps the model utilize more computation during testing to decompose tasks effectively. Additionally, the Tree of Thoughts method, introduced by Yao et al. (2023), extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought processes.\n",
      "\n",
      "Task decomposition can be achieved in various ways, such as through simple prompting by the LLM model, task-specific instructions tailored to the nature of the task, or with human inputs. Regardless of the method used, task decomposition plays a crucial role in enhancing the performance of LLM agents by enabling them to tackle complex tasks in a systematic and efficient manner.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 1. Few-shot examples for generating the step-back question\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel's personal history?\"\n",
    "    },\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "example_prompt=example_prompt, examples=examples\n",
    ")\n",
    "prompt_step_back = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Chain to generate step-back question\n",
    "\n",
    "generate_queries_step_back = prompt_step_back | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "# 3. Response prompt using both normal and step-back context\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question.\n",
    "Your response should be comprehensive and not contradict the following contexts.\n",
    "Normal Context:\n",
    "{normal_context}\n",
    "Step-back Context:\n",
    "{step_back_context}\n",
    "\n",
    "Original Question: {question}\n",
    "Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# 4. Full chain with parallel retrieval\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = chain.invoke({\"question\": question})\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261ca54",
   "metadata": {},
   "source": [
    "### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      " Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals. This enables efficient handling of complex tasks by dividing them into more manageable steps, allowing the agent to navigate through the problem more effectively.\n"
     ]
    }
   ],
   "source": [
    "# 1. Prompt to generate a hypothetical document\n",
    "template = \"\"\"\n",
    "Please write a scientific paper passage to answer the question.\n",
    "Question: {question}\n",
    "Passage:\n",
    "\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Chain to generate the document\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})\n",
    "\n",
    "# 3. Retrieval chain using the hypothetical document\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "\n",
    "# 4. Final RAG chain\n",
    "template = \"\"\"\n",
    "    Answer the following question based on this context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = final_rag_chain.invoke({\"question\": question})\n",
    "print(\"Answer: \\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4265e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
